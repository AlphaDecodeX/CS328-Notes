{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH (Locality Sensitive Hashing) Application\n",
    "\n",
    "\n",
    "---\n",
    "<br/>\n",
    "<h2>Finding Similar Items from a large Dataset with Precesion using LSH.</h2>\n",
    "<br/>\n",
    "\n",
    "<h3>Intro</h3>\n",
    "\n",
    "<p>\n",
    "We want to search similar things from a large dataset. Brute Force approach takes O(N^2) time but LSH cuts this time to O(N)\n",
    "</p>\n",
    "<br/>\n",
    "\n",
    "<h3>Different Scenarios</h3>\n",
    "\n",
    "<p>\n",
    "While searching in the documents we can have two issues<br/><br/>\n",
    "1. We have a reasonable amount of Documents but every document have a large length.\n",
    "2. We have a large number of documents with reasonable length.\n",
    "</p>\n",
    "\n",
    "<h3>Let's Explore the LSH usage in a large Documents Database</h3>\n",
    "\n",
    "<p>First we have to Vectorize the Document. To do that we build k-shingles. And then we make a big set of k-shingles and assign every shingle to an unique index</p>\n",
    "\n",
    "<img src = \"../assets/2022_04_07_Building_LSH_Tables/Shingles.jpeg\"/>\n",
    "\n",
    "<p>Now we have a large document of NXD dimensions with N documents and D columns having values 1/0 representing differnet shinngles. But we can't work with that dense matrix.</p>\n",
    "<p>Before moving further, Let's see the Jacard Matrix</p>\n",
    "\n",
    "$$J = \\frac{A \\cap B}{A \\cup B}$$\n",
    "\n",
    "<span>Now, we can take N documents of D dimensions and can find the Jaccard similarity above some threshold.</span>\n",
    "\n",
    "<p>Now to overcome this issue of iterating through large N documents we use Min-Hashing.</p>\n",
    "\n",
    "<br/>\n",
    "<p>For Example — Let suppose we have A = {1,0,0,1} and B = {0,0,0,1}. and let suppose these are shuffled already, the first index would be A[0]=1 and B[0]=0. A[0]!=B[0] which means the two are not similar and we return 0. Let's do the second shuffle. Shuffle gives A={0,0,1,1}, B={0,0,1,0}, so we get A[2]=B[2]!=0 which is a +1. Ultimately, if you shuffle enough times, you will find the similarity approaches the true value of 0.25. It is an approximation method that yields an expected similarity value. And more the data we have more the accuracy we'll generate.</p>\n",
    "\n",
    "<h3>Generating Unique Hashes</h3>\n",
    "<p>Take two random numbers a and b and pass through the forumula hash(x) = (a*x+b) mod D, where x is the index we are permuting. We generate new hashes by randomly choosing a, b. </p>\n",
    "\n",
    "<h3>The Signature Matrix</h3>\n",
    "<br/>\n",
    "<p>Now we'll compute (N,K) Signature Matrix that has K number of random hash functions. Now we can compare N,K vectors as compared to the large N,D matrix. Signature matrix column represents one random hash functions. For every row in a column, the value is given by the index of the first nonzero element for that row (which is the MinHash).</p>\n",
    "\n",
    "<img src = \"../assets/2022_04_07_Building_LSH_Tables/Signature.jpeg\"/>\n",
    "\n",
    "<br/><br/>\n",
    "<p>Now, To write an efficient algorithm, we don’t actually need to permute all of the rows. It is now okay to just look at every element of the (N,D) dense matrix, and see if it has a 1, and if so if any permutations of the entry lead to a new MinHash.</p>\n",
    "\n",
    "<p>Now instead of iterating on (N,D) we can iterate on (N,K) But it is still O(N^2)</p>\n",
    "\n",
    "<h3>Now to make it Precise we'll use LSH (Locality Sensitive Hashingh)</h3>\n",
    "<br/>\n",
    "<h4>LSH: The band structure</h4>\n",
    "<br/>\n",
    "<p>Let's take an intuition that If we are able to map each row of the matrix to the buckets then the rows that are into the same bucket are somewhat similar or close. The “close” is precisely the desired similarity threshold we introduced earlier.</p>\n",
    "\n",
    "<p>Now, coming to the defination of LSH, It is a family of functions which tune the functions such that points that are close to each other gets closer and points that are not close gets farthest apart with probability approaching to 1</p>\n",
    "\n",
    "<p>Now we will take a band b. Earlier we were trying to match each row with other but now we give each row b possible chances. So we hash each row to b buckets and see who it matches with.</p>\n",
    "\n",
    "<span> ------>  r below is the number of rows per band</span>\n",
    "\n",
    "\n",
    "<p>Now, For better understanding, increasing b gives rows more chances to match, so it let’s in candidate pairs with lower similarity scores. Increasing r makes the match criteria stricter, restricting to higher similarity scores. r and b do opposite things or having a tradeoff between them (means one has opposite effect on the other one).</p>\n",
    "\n",
    "\n",
    "<mark>Now, we can precisely tune the parameters b and r such that we turn the likelihood of a candidate pair into a step function around our desired threshold</mark>\n",
    "\n",
    "- Note: Two rows are considered candidate pairs which means that they have the possibility to have a similarity score above the desired threshold, if the two rows have at least one band in which all the rows are identical.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "710c0831ed8213cd065c358984800958cf00e4cbcb817e6ea96da36664e27237"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
